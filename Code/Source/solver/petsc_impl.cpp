
/* Copyright (c) Stanford University, The Regents of the University of California, and others.
 *
 * All Rights Reserved.
 *
 * See Copyright-SimVascular.txt for additional details.
 *
 * Permission is hereby granted, free of charge, to any person obtaining
 * a copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject
 * to the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Software.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS
 * IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
 * PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER
 * OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
 * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
 * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
 * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

// This file contains PetscImpl and PETSc-dependent functions. 

#include "petsc_impl.h"
#include <locale.h>

LHSCtx plhs;       /* PETSc lhs */
LSCtx *psol;       /* PETSc solver */
PetscLogStage stages[6];  /* performance tuning. */

/*
    Nomenclature used in this file:
    - global: across all MPI processes
    - local: current MPI processes
    - natural id: global vertex id from mesh file
    - PETSc id: global vertex id used within PETSc
    - O1 ordering: the local order of natural id generated by ParMetis in svFSI
    - O2 ordering: reorder O1 to "(vertex shared with lower rank)  + 
                   (vertex owned exclusively by current rank) + 
                   (vertex shared with higher rank)"
    - owned vertices: "(vertex shared with lower rank)  + 
                      (vertex owned exclusively by current rank)" 
                      forms the vertices owned by the current rank.
    - ghost vertices: vertex shared with higher rank 

    The mapping between different ordering is
        O1 --> O2 --> PETSc ID
*/

// Initialize PETSc and create lhs for PETSc.
//
void petsc_initialize(const PetscInt nNo, const PetscInt mynNo, const PetscInt nnz, 
    const PetscInt nEq, const PetscInt *svFSI_ltg, const PetscInt *svFSI_map, 
    const PetscInt *svFSI_rowPtr, const PetscInt *svFSI_colPtr, char *inp)
{   
    // char* in_file = rm_blank(inp);
    char* in_file = inp;
    if (access(in_file, F_OK) == 0) {
        PetscInitialize(NULL, NULL, in_file, NULL);
        PetscPrintf(MPI_COMM_WORLD, " <PETSC_INITIALIZE>: "
                "use linear solver config. from file.\n");
    }
    else {
        PetscInitialize(NULL, NULL, NULL, NULL);
        PetscPrintf(MPI_COMM_WORLD, " <PETSC_INITIALIZE>: "
                "use linear solver config. from svFSI input.\n");
    }
    
    PetscLogStageRegister("Create LHS", &stages[0]);
    PetscLogStageRegister("Create VecMat", &stages[1]);
    PetscLogStageRegister("Create Solver", &stages[2]);
    PetscLogStageRegister("Set Values", &stages[3]);
    PetscLogStageRegister("PETSc Solve", &stages[4]);
    PetscLogStageRegister("Row-Col. Sca.", &stages[5]);

    PetscLogStagePush(stages[0]);

    plhs.created = PETSC_FALSE;
    petsc_create_lhs(nNo, mynNo, nnz, svFSI_ltg, svFSI_map, svFSI_rowPtr, svFSI_colPtr);

    PetscMalloc1(nEq, &psol);

    for (PetscInt i = 0; i < nEq; i++) {
      psol[i].created = PETSC_FALSE;
    }

    PetscLogStagePop();
}

/*
    Create parallel vector and matrix data structures.
*/
void petsc_create_linearsystem(const PetscInt dof, const PetscInt iEq, const PetscInt nEq, 
    const PetscReal *svFSI_DirBC, const PetscReal *svFSI_lpBC)
{   
    // PetscInt cEq = *iEq - 1;
    PetscInt cEq = iEq;   // in Fortran, cEq = 1; in C++, cEq = 0;

    if (psol[cEq].created) {
      return;
    }

    PetscLogStagePush(stages[1]);
    petsc_create_bc(dof, cEq, svFSI_DirBC, svFSI_lpBC); /* bc info is required for mat_create */
    petsc_create_vecmat(dof, cEq, nEq);
    psol[cEq].created = PETSC_TRUE;
    PetscLogStagePop();
}

/*
    Create PETSc linear solver data.
*/
void petsc_create_linearsolver(const consts::SolverType lsType, const consts::PreconditionerType pcType, 
    const PetscInt kSpace, const PetscInt maxIter, const PetscReal relTol, const PetscReal absTol, 
    const consts::EquationType phys, const PetscInt dof, const PetscInt iEq, const PetscInt nEq)
{   
    using namespace consts;

    // PetscInt cEq = *iEq - 1;
    PetscInt cEq = iEq;   // in Fortran, cEq = 1; in C++, cEq = 0;
    PC pc;
    PetscBool usefieldsplit;

    PetscLogStagePush(stages[2]);

    /* Initialize equation specific prefix for vec/mat/ksp */
    switch (phys)
    {
        case EquationType::phys_fluid:
            psol[cEq].pre = "ns_";
            break;
        case EquationType::phys_struct:
            psol[cEq].pre = "st_";
            break;
        case EquationType::phys_heatS:
            psol[cEq].pre = "hs_";
            break;
        case EquationType::phys_lElas:
            psol[cEq].pre = "le_";
            break;
        case EquationType::phys_heatF:
            psol[cEq].pre = "hf_";
            break;
        case EquationType::phys_FSI:
            psol[cEq].pre = "fs_";
            break;
        case EquationType::phys_mesh:
            psol[cEq].pre = "ms_";
            break;
        case EquationType::phys_shell:
            psol[cEq].pre = "sh_";
            break;
        case EquationType::phys_CMM:
            psol[cEq].pre = "cm_";
            break;
        case EquationType::phys_CEP:
            psol[cEq].pre = "ep_";
            break;
        case EquationType::phys_ustruct:
            psol[cEq].pre = "st_";
            break;
        case EquationType::phys_stokes:
            psol[cEq].pre = "ss_";
            break;
        default:
            PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_CREATE_LINEARSOLVER>: "
                "equation type %d is not defined.\n", phys);
            break;
    }

    /* Initialize PETSc linear solver setting */
    KSPCreate(MPI_COMM_WORLD, &psol[cEq].ksp);

    if (nEq > 1) {
      KSPSetOptionsPrefix(psol[cEq].ksp, psol[cEq].pre);
    }

    KSPSetTolerances(psol[cEq].ksp, relTol, absTol, PETSC_DEFAULT, maxIter); 
    
    // Set the linear solver.

    switch (lsType) {
        case SolverType::lSolver_CG:
            KSPSetType(psol[cEq].ksp, KSPCG);
            break;
        case SolverType::lSolver_GMRES:
            KSPSetType(psol[cEq].ksp, KSPGMRES);
//            KSPGMRESSetRestart(psol[cEq].ksp, *kSpace);
            break;
        case SolverType::lSolver_BICGS:
            KSPSetType(psol[cEq].ksp, KSPBCGS);
            break;
        case SolverType::lSolver_FGMRES:
            KSPSetType(psol[cEq].ksp, KSPFGMRES);
            break;
        default:
            PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_CREATE_LINEARSOLVER>: "
            "linear solver type not supported through svFSI input file.\n"
            "ERROR <PETSC_CREATE_LINEARSOLVER>: "
            "More linear solver types can be set through petsc_option.inp.\n");
            break;
    }
    
    /* Set preconditioner */
    psol[cEq].bnpc = PETSC_FALSE;
    psol[cEq].rcs = PETSC_FALSE;
    KSPGetPC(psol[cEq].ksp, &pc);

    switch (pcType)
    {
        case PreconditionerType::PREC_PETSC_JACOBI:
            PCSetType(pc, PCJACOBI);
        break;

        case PreconditionerType::PREC_PETSC_NESTEDBLOCK: // To be used with FGMRES solver
            psol[cEq].bnpc = PETSC_TRUE;
            PCSetType(pc, PCSHELL);

            const double default_rtol0 = 1.0e-5;
            const double default_atol0 = 1.0e-50;
            const double default_dtol0 = 1.0e50;
            const int default_maxit0 = 10000;

            const double default_rtol1 = 1.0e-5;
            const double default_atol1 = 1.0e-50;
            const double default_dtol1 = 1.0e50;
            const int default_maxit1 = 10000;

            // Create the block nested preconditioner context object
            BlockNestedPreconditioner *block_nested_pc_context = new BlockNestedPreconditioner(
                default_rtol0, default_atol0, default_dtol0, default_maxit0,
                default_rtol1, default_atol1, default_dtol1, default_maxit1);

            // Attach the context to the PCSHELL
            PCShellSetContext(pc, block_nested_pc_context);
            PCShellSetApply(pc, [](PC pc, Vec r, Vec z) -> PetscErrorCode {
                BlockNestedPreconditioner *ctx;
                PCShellGetContext(pc, &ctx);
                return ctx->BlockNestedPC_Apply(pc, r, z);
            });
            PCShellSetDestroy(pc, [](PC pc) -> PetscErrorCode {
                BlockNestedPreconditioner *ctx;
                PCShellGetContext(pc, &ctx);
                delete ctx;
                return 0;
            });
        break;

        case PreconditionerType::PREC_PETSC_RCS:
            psol[cEq].rcs = PETSC_TRUE;
            PetscPrintf(MPI_COMM_WORLD, "WARNING <PETSC_CREATE_LINEARSOLVER>: "
            "precondition the linear system with RCS first.\n"
            "WARNING <PETSC_CREATE_LINEARSOLVER>: "
            "This will NOT be overwritten by petsc_option.inp!\n");
        break;

        default:
            PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_CREATE_LINEARSOLVER>: "
            "preconditioner type not supported through svFSI input file.\n"
            "ERROR <PETSC_CREATE_LINEARSOLVER>: "
            "More preconditioner options can be set through petsc_option.inp.\n");   
        break;
    }
    
    /* Run time options */
    KSPSetFromOptions(psol[cEq].ksp);

    /* Set up PCFIELDSPLIT */
    PetscObjectTypeCompare((PetscObject)pc, PCFIELDSPLIT, &usefieldsplit);
    if (usefieldsplit){
        if (phys==EquationType::phys_fluid || phys==EquationType::phys_ustruct || phys==EquationType::phys_stokes ){
            petsc_set_pcfieldsplit(dof, cEq);
        }
        else {
            PetscPrintf(MPI_COMM_WORLD, "///////////////////////////////////"
            "///////////////////////////////////////////////////////////////\n");
            PetscPrintf(MPI_COMM_WORLD, "WARNING <PETSC_CREATE_LINEARSOLVER>: "
            "PCFIELDSPLIT is hard-coded for ustruct/stokes/NS.\n");
            PetscPrintf(MPI_COMM_WORLD, "///////////////////////////////////"
            "///////////////////////////////////////////////////////////////\n");
        }
    }

    PetscLogStagePop();
}

/*
    Set up the linear system.
*/
void petsc_set_values(const PetscInt dof, const PetscInt iEq, const PetscReal *R, 
    const PetscReal *Val, const PetscReal *svFSI_DirBC, const PetscReal *svFSI_lpBC)
{   
    // PetscInt cEq = *iEq - 1;
    PetscInt cEq = iEq;   // in Fortran, cEq = 1; in C++, cEq = 0;

    /* Set values in A &b, apply Dir and Lumped parameter BC */
    PetscLogStagePush(stages[3]);
    petsc_set_vec(dof, cEq, R);
    petsc_set_mat(dof, cEq, Val);
    petsc_set_bc(cEq, svFSI_DirBC, svFSI_lpBC);
    PetscLogStagePop();

    /* Scale A and b if RCS preconditioner is activated. */
    PetscLogStagePush(stages[5]);
    if (psol[cEq].rcs){
      petsc_pc_rcs(dof, cEq);
    }
    PetscLogStagePop();
}

/*
    Solve the linear system.
*/
void petsc_solve(PetscReal *resNorm,  PetscReal *initNorm,  PetscReal *dB, 
    PetscReal *execTime, bool *converged, PetscInt *numIter, 
    PetscReal *R, const PetscInt maxIter, const PetscInt dof, 
    const PetscInt iEq)
{   
    PetscReal *a, *array;
    PetscInt   i, j, na;
    // PetscInt   cEq = *iEq - 1;
    PetscInt   cEq = iEq;   // in Fortran, cEq = 1; in C++, cEq = 0;
    PetscBool  usepreonly;
    KSPType    ksptype;
    Vec        lx, res;
    KSPConvergedReason reason;
    PetscLogDouble ts, te;

    PetscLogStagePush(stages[4]);
    na = maxIter;
    PetscMalloc1(na, &a);
    PetscTime(&ts);
    KSPSetOperators(psol[cEq].ksp, psol[cEq].A, psol[cEq].A);

    // Update the PCSHELL context with the tangent matrix
    if (psol[cEq].bnpc) {
        Mat Amat;
        BlockNestedPreconditioner *ctx;
        PCShellGetContext(psol[cEq].pc, &ctx);
        KSPGetOperators(psol[cEq].ksp, &Amat, NULL)
        ctx->BlockNestedPC_SetMatrix(Amat);
    }

    /* Calculate residual for direct solver. KSP uses preconditioned norm. */
    PetscObjectTypeCompare((PetscObject)psol[cEq].ksp, KSPPREONLY, &usepreonly);

    if (usepreonly){
      VecNorm(psol[cEq].b, NORM_2 , initNorm);
    } else {
      KSPSetResidualHistory(psol[cEq].ksp, a, na, PETSC_TRUE);
    } 

    KSPSetUp(psol[cEq].ksp);
    KSPSolve(psol[cEq].ksp, psol[cEq].b, psol[cEq].b);

    /* Rescale solution for RCS preconditioner */
    if (psol[cEq].rcs){
        VecPointwiseMult(psol[cEq].b, psol[cEq].b, psol[cEq].Dc);
    }

    /* Fill the ghost vertices with correct values. */
    VecGhostUpdateBegin(psol[cEq].b, INSERT_VALUES, SCATTER_FORWARD);
    VecGhostUpdateEnd(psol[cEq].b, INSERT_VALUES, SCATTER_FORWARD);

    PetscTime(&te);

    /* Get convergence info. */
    if (usepreonly){
        *resNorm   = __DBL_EPSILON__;
    }
    else {
        KSPGetResidualHistory(psol[cEq].ksp, (const PetscReal **) &a, &na);
        *initNorm  = a[0];
        *resNorm   = a[na-1];
    }
    KSPGetIterationNumber(psol[cEq].ksp, numIter);
    KSPGetConvergedReason(psol[cEq].ksp, &reason);
    *converged = reason > 0 ? true : false;
    *dB        = 10.0 * log(*resNorm / *initNorm);
    *execTime  = te - ts;

    /* Export solution to svFSI. */
    VecGhostGetLocalForm(psol[cEq].b, &lx);
    VecGetArray(lx, &array);
    na = 0;
    for (i = 0; i < plhs.nNo; i++) {
        for (j = 0; j < dof; j++) {
            R[plhs.map[i]*(dof)+j] = array[na++];
        }
    }
    VecRestoreArray(lx, &array);
    VecGhostRestoreLocalForm(psol[cEq].b, &lx);

    PetscFree(a);
    PetscLogStagePop();
}

/* 
    Clean up all petsc data. 
*/
void petsc_destroy_all(const PetscInt nEq)
{   
    if (!psol==NULL){
        PetscInt cEq; 
        PetscErrorCode ierr;

        if (!plhs.created) {
            PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_DESTROY_ALL>: "
                    "lhs is not created.\n");
            ierr = PETSC_ERR_ARG_WRONGSTATE;
            PETSCABORT(MPI_COMM_WORLD, ierr);
        }

        plhs.nNo     = 0;
        plhs.mynNo   = 0;
        plhs.created = PETSC_FALSE;

        PetscFree (plhs.map);
        PetscFree2(plhs.rowPtr, plhs.colPtr);
        PetscFree2(plhs.ltg, plhs.ghostltg);

        for (cEq = 0; cEq < nEq; cEq++)
        {   
            if (!psol[cEq].created) {
                PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_DESTROY_ALL>: "
                    "solver %d is not created.\n", cEq);
                ierr = PETSC_ERR_ARG_WRONGSTATE;
                PETSCABORT(MPI_COMM_WORLD, ierr);
            }

            psol[cEq].created = PETSC_FALSE;

            psol[cEq].lpPts = 0;
            PetscFree2(psol[cEq].lpBC_l, psol[cEq].lpBC_g);

            psol[cEq].DirPts = 0;
            PetscFree (psol[cEq].DirBC);

            VecDestroy(&psol[cEq].b);
            MatDestroy(&psol[cEq].A);
            KSPDestroy(&psol[cEq].ksp);

            if (psol[cEq].bnpc) {
                psol[cEq].bnpc = PETSC_FALSE;
            }

            if (psol[cEq].rcs) {
                psol[cEq].rcs = PETSC_FALSE;
                VecDestroy(&psol[cEq].Dr);
                VecDestroy(&psol[cEq].Dc);
            }
        }
        PetscFree(psol); 
        
        PetscFinalize();
    }
}


/*
    Creating PETSc lhs data structure with svFSI info.
*/
PetscErrorCode petsc_create_lhs(const PetscInt nNo, const PetscInt mynNo, const PetscInt nnz, \
                                const PetscInt *svFSI_ltg, const PetscInt *svFSI_map, \
                                const PetscInt *svFSI_rowPtr, const PetscInt *svFSI_colPtr)
{   
    PetscInt  i, j ;
    PetscErrorCode ierr;
    PetscInt *local2global, *local2local; /* local copy of svFSI ltg and map */
    PetscInt *local_ltg;                  /* local to global mapping of all vertices on current proc.*/
    PetscInt *owned_ltg;                  /* local to global mapping of owned vertices */
    PetscInt *ghost_ltg;                  /* local to global mapping of ghost vertices */
    PetscInt  rstart;                     /* starting index of PETSc ordering for a processor */
    AO        ao;                         /* Application Ordering object */
    PetscInt *pordering;                  /* PETSc ordering */
    PetscInt  ghostnNo;                   /* number of ghost vertices */

    PetscFunctionBeginUser;

    /* In cases with remeshing, lhs needs to be regenerated. */
    if (plhs.created) {
        PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_DESTROY_ALL>: \
                lhs is already created.");
        ierr = PETSC_ERR_ARG_WRONGSTATE;
        PETSCABORT(MPI_COMM_WORLD, ierr);
    }

    plhs.nNo     = nNo;
    plhs.mynNo   = mynNo;
    plhs.created = PETSC_TRUE;

    /* Fortran index to C index (NOT apply for svFSIplus) */ 
    PetscCall(PetscMalloc2(nNo, &local2global, nNo, &local2local));
    for (i = 0; i < nNo; i++) {
        // local2global[i] = svFSI_ltg[i] - 1;
        // local2local[i]  = svFSI_map[i] - 1;
        local2global[i] = svFSI_ltg[i];
        local2local[i]  = svFSI_map[i];
    }

    /* Create local mapping, map[O2] = O1 */
    PetscCall(PetscMalloc1(nNo, &plhs.map));
    for (i = 0; i < nNo; i++) {
        plhs.map[local2local[i]] = i;
    }

    /* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 
        Create global-to-global mapping using AO:
        - Global order 1 is from svFSI (ltg, i.e. O1).
        - Global order 2 is for PETSc (PETSc ordering). It is:
            |---- Proc 0-------|   |----------- Proc 2 ------------|
            0 1 ...   mynNo(0)-1    mynNo(0) ... mynNo(0)+mynNo(1)-1
        
        - Note that global order 2 locally follows O2 ordering, i.e.
           lower rank vtx + current rank vtx 
        - Vertices from higher rank are ghost vertices.
     - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */
    /* Remap svFSI ltg to svFSI lhs%map order (O2) */
    PetscCall(PetscMalloc2(mynNo, &owned_ltg, nNo, &local_ltg));
    for (i = 0; i < nNo; i++) {
        j = local2local[i];
        local_ltg[j] = local2global[i];
    }
    for (i = 0; i < mynNo; i++) owned_ltg[i] = local_ltg[i];
    ghostnNo = nNo - mynNo;
    PetscCall(PetscMalloc1(ghostnNo, &ghost_ltg));
    for (i = 0; i < ghostnNo; i++) ghost_ltg[i] = local_ltg[i+mynNo];

    /* Create AO object between svFSI ltg and PETSc ordering */
    PetscCallMPI(MPI_Scan(&mynNo, &rstart, 1, MPIU_INT, MPI_SUM, MPI_COMM_WORLD));
    rstart -= mynNo;
    PetscCall(PetscMalloc1(mynNo, &pordering));
    for (i = 0; i < mynNo; i++) pordering[i] = rstart + i;
    PetscCall(AOCreateBasic(MPI_COMM_WORLD, mynNo, owned_ltg, pordering, &ao));

    /* 
        Now map the vertex id in natural ordering to PETSc ordering.
        Before AOApplicationToPetsc:
            local_ltg[i] = natural id, i = 0,..., nNo-1
        After AOApplicationToPetsc:
            local_ltg[i] = PETSc id, i = 0,..., nNo-1
    */
    PetscCall(AOApplicationToPetsc(ao, nNo, local_ltg));
    PetscCall(AOApplicationToPetsc(ao, ghostnNo, ghost_ltg));
    PetscCall(AODestroy(&ao));

    PetscCall(PetscMalloc2(nNo, &plhs.ltg, ghostnNo, &plhs.ghostltg));
    for (i = 0; i < nNo; i++) plhs.ltg[i] = local_ltg[i];
    for (i = 0; i < ghostnNo; i++) plhs.ghostltg[i] = ghost_ltg[i];

    /* Adjacency info in PETSc lhs (i.e plhs.rowPtr and plhs.colPtr) is used to set values. */
    PetscCall(PetscMalloc2(2*nNo, &plhs.rowPtr, nnz, &plhs.colPtr));
    for (i=0; i < nNo; i++) {
        // plhs.rowPtr[i*2]   = svFSI_rowPtr[i*2] - 1; 
        plhs.rowPtr[i*2]   = svFSI_rowPtr[i*2];
        // plhs.rowPtr[i*2+1] = svFSI_rowPtr[i*2+1];
        plhs.rowPtr[i*2+1] = svFSI_rowPtr[i*2+1]+1;
    }
    for (i=0; i < nnz; i++) {
        // plhs.colPtr[i] = plhs.ltg[svFSI_colPtr[i] - 1];
        plhs.colPtr[i] = plhs.ltg[svFSI_colPtr[i]];
    }
    
    /* Deallocate memory */
    PetscCall(PetscFree2(owned_ltg, local_ltg));
    PetscCall(PetscFree2(local2global, local2local));
    PetscCall(PetscFree(ghost_ltg));
    PetscCall(PetscFree(pordering));

    PetscFunctionReturn(PETSC_SUCCESS);
}

/*
    Creating PETSc data structure for Dirichlet and lumped parameter BC with svFSI info.
*/
PetscErrorCode petsc_create_bc(const PetscInt dof, const PetscInt cEq, \
                               const PetscReal *svFSI_DirBC, const PetscReal *svFSI_lpBC)
{   
    PetscInt  i, j, cc, ii;
    PetscInt *row1, *row2;
    PetscReal eps = 1.0e6*__DBL_EPSILON__;

    PetscFunctionBeginUser;

    /* Find PETSc global index of dofs with Dirichlet BC */
    PetscCall(PetscMalloc1(plhs.mynNo*dof, &row1));
    cc = 0;
    for (i = 0; i < plhs.mynNo; i++) {
        ii = i * dof;
        for (j = 0; j < dof; j++) {
            if ( ((int) svFSI_DirBC[ii+j]) == 0) {
                row1[cc++] = plhs.ltg[i]*dof + j;
            }
        }
    }
    PetscCall(PetscMalloc1(cc, &psol[cEq].DirBC));
    psol[cEq].DirPts = cc;
    for (i = 0; i < cc; i++){
        psol[cEq].DirBC[i] = row1[i];
    }

    /* Find O2 and PETSc index of dofs with lumped parameter BC */
    PetscCall(PetscMalloc1(plhs.mynNo*dof, &row2));
    cc = 0;
    for (i = 0; i < plhs.mynNo; i++) {
        ii = i * dof;
        for (j = 0; j < dof; j++) {
            if ( PetscAbsReal(svFSI_lpBC[ii+j]) > eps) {
                row1[cc] = ii + j;
                row2[cc] = plhs.ltg[i]*dof + j;
                cc++;
            }
        }
    }
    PetscCall(PetscMalloc2(cc, &psol[cEq].lpBC_l, cc, &psol[cEq].lpBC_g));
    psol[cEq].lpPts = cc;
    for (i = 0; i < cc; i++){
        psol[cEq].lpBC_l[i] = row1[i];
        psol[cEq].lpBC_g[i] = row2[i];
    }
    PetscCall(PetscFree(row1));
    PetscCall(PetscFree(row2));

    PetscFunctionReturn(PETSC_SUCCESS);
}

/*
    Create and preallocate parallel PETSC vector and matrix data structure.
*/
PetscErrorCode petsc_create_vecmat(const PetscInt dof, const PetscInt cEq, const PetscInt nEq)
{   
    PetscInt   i, j, is, ie, row, col;
    Mat        preallocator;
    PetscReal *value;
    PC         pc;
    PetscBool  usefieldsplit, useamg;

    PetscFunctionBeginUser;

    /* Create vector data structures */
    PetscCall(VecCreateGhostBlock(MPI_COMM_WORLD, dof, plhs.mynNo*dof, PETSC_DECIDE, plhs.nNo-plhs.mynNo, plhs.ghostltg, &psol[cEq].b));
    if (nEq > 1) PetscCall(VecSetOptionsPrefix(psol[cEq].b, psol[cEq].pre));
    PetscCall(VecSetFromOptions(psol[cEq].b));


    /*
        Preallocate psol.A with the help of MATPREALLOCATOR.
        Internally MatPreallocatorPreallocate() will call MatSetOption(A, MAT_NEW_NONZERO_ALLOCATION_ERR, PETSC_TRUE);
    */
    PetscCall(MatCreate(MPI_COMM_WORLD, &preallocator));
    PetscCall(MatSetType(preallocator, MATPREALLOCATOR));
    PetscCall(MatSetSizes(preallocator, plhs.mynNo*dof, plhs.mynNo*dof, PETSC_DECIDE, PETSC_DECIDE));
    PetscCall(MatSetBlockSize(preallocator, dof));
    PetscCall(MatSetUp(preallocator));
    PetscCall(PetscMalloc1(dof*dof, &value));
    for (i = 0; i < dof*dof; i++) value[i] = 0.0;
    /* Internal points */
    for (i = 0; i < plhs.nNo; i++) {
        is  = plhs.rowPtr[i*2];
        ie  = plhs.rowPtr[i*2+1];
        row = plhs.ltg[i];
        for (j = is; j < ie; j++) {
            col = plhs.colPtr[j];
            PetscCall(MatSetValuesBlocked(preallocator, 1, &row, 1, &col, value, INSERT_VALUES));
        }
    }
    /* Points with lumped parameter BC */
    for (i = 0; i < psol[cEq].lpPts; i++){
        row = psol[cEq].lpBC_g[i];
        for (j = 0; j < psol[cEq].lpPts; j++){
            col = psol[cEq].lpBC_g[j];
            PetscCall(MatSetValue(preallocator, row, col, 0.0, INSERT_VALUES));
        }
    }
    PetscCall(MatAssemblyBegin(preallocator, MAT_FINAL_ASSEMBLY));
    PetscCall(MatAssemblyEnd(preallocator, MAT_FINAL_ASSEMBLY));

    /* Create and preallocate matrix structure */
    KSPGetPC(psol[cEq].ksp, &pc);
    PetscObjectTypeCompare((PetscObject)pc, PCFIELDSPLIT, &usefieldsplit); /* Fieldsplit only supports MATAIJ */
    PetscObjectTypeCompare((PetscObject)pc, PCGAMG, &useamg); /* GAMG only supports MATAIJ */
    PetscCall(MatCreate(MPI_COMM_WORLD, &psol[cEq].A));
    if (dof > 1 && !usefieldsplit && !useamg) {
        PetscCall(MatSetType(psol[cEq].A, MATBAIJ));
    }
    else {
        PetscCall(MatSetType(psol[cEq].A, MATAIJ));
    }
    if (nEq > 1) PetscCall(MatSetOptionsPrefix(psol[cEq].A, psol[cEq].pre));
    PetscCall(MatSetFromOptions(psol[cEq].A));
    PetscCall(MatSetSizes(psol[cEq].A, plhs.mynNo*dof, plhs.mynNo*dof, PETSC_DECIDE, PETSC_DECIDE));
    PetscCall(MatSetBlockSize(psol[cEq].A, dof));
    PetscCall(MatPreallocatorPreallocate(preallocator, PETSC_TRUE, psol[cEq].A));
    PetscCall(MatSetOption(psol[cEq].A, MAT_NEW_NONZERO_LOCATION_ERR, PETSC_TRUE));
    PetscCall(MatSetOption(psol[cEq].A, MAT_NEW_NONZERO_LOCATIONS, PETSC_FALSE));

    /* Create vector structure for RCS preconditioner */
    if (psol[cEq].rcs){
        PetscCall(VecDuplicate(psol[cEq].b, &psol[cEq].Dr));
        PetscCall(VecDuplicate(psol[cEq].b, &psol[cEq].Dc));
    }

    PetscCall(PetscFree(value));
    PetscCall(MatDestroy(&preallocator));

    PetscFunctionReturn(PETSC_SUCCESS);
}

/*
    Set values to the rhs vector.
    R is the rhs from svFSI in O1 order.
*/
PetscErrorCode petsc_set_vec(const PetscInt dof, const PetscInt cEq, const PetscReal *R)
{
    PetscInt   indx, i;
    PetscInt  *row;
    const PetscReal *value;

    PetscFunctionBeginUser;

    /* Set the rhs vector using global index. */
    PetscCall(VecZeroEntries(psol[cEq].b));

    for (i = 0; i < plhs.mynNo; i++) {
        row   = plhs.ltg+i;
        indx  = plhs.map[i];
        value = R + indx*dof;
        PetscCall(VecSetValuesBlocked(psol[cEq].b, 1, row, value, INSERT_VALUES));
    }
    PetscCall(VecAssemblyBegin(psol[cEq].b));
    PetscCall(VecAssemblyEnd(psol[cEq].b));

    /* Fill the ghost vertices with correct values. */
    PetscCall(VecGhostUpdateBegin(psol[cEq].b, INSERT_VALUES, SCATTER_FORWARD));
    PetscCall(VecGhostUpdateEnd(psol[cEq].b, INSERT_VALUES, SCATTER_FORWARD));
    
    PetscFunctionReturn(PETSC_SUCCESS);
}

/*
    Set values to the matrix.
*/
PetscErrorCode petsc_set_mat(const PetscInt dof, const PetscInt cEq, const PetscReal *Val)
{   
    PetscInt   i, j, is, ie; 
    PetscInt  *col, *row;
    const PetscScalar *value;

    PetscFunctionBeginUser;

    PetscCall(MatZeroEntries(psol[cEq].A));
    for (i = 0; i < plhs.nNo; i++) {
        is    = plhs.rowPtr[i*2];
        ie    = plhs.rowPtr[i*2+1];
        row   = plhs.ltg+i;
        for (j = is; j < ie; j++){
            value = Val + j*dof*dof;
            col   = plhs.colPtr + j;
            PetscCall(MatSetValuesBlocked(psol[cEq].A, 1, row, 1, col, value, ADD_VALUES));  
        }
        
    }
    PetscCall(MatAssemblyBegin(psol[cEq].A, MAT_FLUSH_ASSEMBLY));
    PetscCall(MatAssemblyEnd(psol[cEq].A, MAT_FLUSH_ASSEMBLY));

    PetscFunctionReturn(PETSC_SUCCESS);
}

/*
    Set up Dirichlet BC and resistance BC.
*/
PetscErrorCode petsc_set_bc(const PetscInt cEq, const PetscReal *DirBC, const PetscReal *lpBC)
{   
    Vec x;
    PetscInt  i, j, ii, jj, row, col;
    PetscReal value;

    PetscFunctionBeginUser;

    /* Apply lumped parameter BC by augmenting the matrix A. */
    for (i = 0; i < psol[cEq].lpPts; i++){
        ii  = psol[cEq].lpBC_l[i];
        row = psol[cEq].lpBC_g[i];
        for (j = 0; j < psol[cEq].lpPts; j++){
            jj  = psol[cEq].lpBC_l[j];
            col = psol[cEq].lpBC_g[j];
            value = lpBC[ii] * lpBC[jj];
            PetscCall(MatSetValue(psol[cEq].A, row, col, value, ADD_VALUES));
        }
    }
    PetscCall(MatAssemblyBegin(psol[cEq].A, MAT_FINAL_ASSEMBLY));
    PetscCall(MatAssemblyEnd(psol[cEq].A, MAT_FINAL_ASSEMBLY));

    /*
        Apply Dirichlet BC by resetting matrix A and rhs b.
        Since the BC remains the same, the matrix will retain the same nonzero structure
    */
    PetscCall(VecDuplicate(psol[cEq].b, &x));
    PetscCall(VecPlaceArray(x, DirBC));
    PetscCall(MatZeroRowsColumns(psol[cEq].A, psol[cEq].DirPts, psol[cEq].DirBC, 1.0, x , psol[cEq].b));

    PetscCall(VecDestroy(&x)); 
    PetscFunctionReturn(PETSC_SUCCESS);
}

/*
    Set up PCFIELDSPLIT.
*/
PetscErrorCode petsc_set_pcfieldsplit(const PetscInt dof, const PetscInt cEq)
{   
    IS        isu, isp;
    PetscInt  i, j, ii, jj;
    PetscInt *uindx, *pindx;
    PC pc;

    PetscFunctionBeginUser;

    /* Create index set for velocity and pressure block */
    PetscCall(PetscMalloc2((dof-1)*plhs.mynNo, &uindx, plhs.mynNo, &pindx));
    for (i = 0; i < plhs.mynNo; i++){
        ii = (dof-1)*i;
        jj = dof*plhs.ltg[i];
        for (j = 0; j < dof-1; j++){
            uindx[ii+j] = jj + j;
        }
        pindx[i] = jj + dof - 1;
    }
    PetscCall(ISCreateGeneral(MPI_COMM_WORLD, (dof-1)*plhs.mynNo, uindx, PETSC_COPY_VALUES, &isu));
    PetscCall(ISSetBlockSize(isu, dof-1));
    PetscCall(ISSort(isu));
    PetscCall(ISCreateGeneral(MPI_COMM_WORLD, plhs.mynNo, pindx, PETSC_COPY_VALUES, &isp));
    PetscCall(ISSort(isp));

    /* Pass split information to PETSc. Velocity: block 0, pressure: block 1. */
    PetscCall(KSPGetPC(psol[cEq].ksp, &pc));
    PetscCall(PCFieldSplitSetBlockSize(pc, 2));
    PetscCall(PCFieldSplitSetIS(pc, "0", isu));
    PetscCall(PCFieldSplitSetIS(pc, "1", isp));

    PetscCall(ISDestroy(&isu));
    PetscCall(ISDestroy(&isp));
    PetscCall(PetscFree2(uindx, pindx));
    PetscFunctionReturn(PETSC_SUCCESS);
}

/*
    Row-and-Column-Scaling preconditioner.
    \hat{A} = Dr*A*Dc
    \hat{b} = Dr*b
*/
PetscErrorCode petsc_pc_rcs(const PetscInt dof, const PetscInt cEq)
{   
    Vec        Dr, Dc;
    PetscInt   numCol, *row;
    PetscReal *colNorm, *value;
    PetscReal  tol, err1, err2;
    PetscInt   i, iter, maxiter;

    tol     = 0.1;
    maxiter = 10;


    PetscFunctionBeginUser;

    PetscCall(VecSet(psol[cEq].Dr, 1.0));
    PetscCall(VecSet(psol[cEq].Dc, 1.0));
    PetscCall(VecDuplicate(psol[cEq].Dr, &Dr));
    PetscCall(VecDuplicate(psol[cEq].Dc, &Dc));


    for (iter = 0; iter < maxiter; iter++){
        /* Get infinity norm of each row */
        PetscCall(MatGetRowMaxAbs(psol[cEq].A, Dr, NULL));

        /* Get infinity norm of each column */
        PetscCall(MatGetSize(psol[cEq].A, NULL, &numCol));
        PetscCall(PetscMalloc1(numCol, &colNorm));
        PetscCall(MatGetColumnNorms(psol[cEq].A, NORM_INFINITY, colNorm));
        for (i = 0; i < plhs.mynNo; i++) {
            row   = plhs.ltg+i;
            value = colNorm + plhs.ltg[i]*dof;
            PetscCall(VecSetValuesBlocked(Dc, 1, row, value, INSERT_VALUES));
        }
        PetscCall(VecAssemblyBegin(Dc));
        PetscCall(VecAssemblyEnd(Dc));

        /* Calculate 1/sqrt(Dr, Dc) */
        PetscCall(VecSqrtAbs(Dc));
        PetscCall(VecReciprocal(Dc));
        PetscCall(VecSqrtAbs(Dr));
        PetscCall(VecReciprocal(Dr));

        /* Scale A matrix */
        PetscCall(MatDiagonalScale(psol[cEq].A, Dr, Dc));
        PetscCall(VecPointwiseMult(psol[cEq].Dc, psol[cEq].Dc, Dc));
        PetscCall(VecPointwiseMult(psol[cEq].Dr, psol[cEq].Dr, Dr));

        /* Converged or not */
        PetscCall(VecShift(Dc, -1.0));
        PetscCall(VecShift(Dr, -1.0));
        PetscCall(VecNorm(Dc, NORM_INFINITY, &err1));
        PetscCall(VecNorm(Dr, NORM_INFINITY, &err2));
        if (err1 <= tol && err2 <= tol) break;
    }

    if (err1 > tol || err2 > tol ) {
        PetscPrintf(MPI_COMM_WORLD, "ERROR <PETSC_PC_RCS>: "
                "did not converge to %g in %d iterations.\n"
                "ERROR <PETSC_PC_RCS>: "
                "err1 = %g, err2 = %g\n", tol, maxiter, err1, err2);
    }

    /* Scale right hand side */
    PetscCall(VecPointwiseMult(psol[cEq].b, psol[cEq].b, psol[cEq].Dr));

    PetscCall(PetscFree(colNorm));
    PetscCall(VecDestroy(&Dr));
    PetscCall(VecDestroy(&Dc));

    PetscFunctionReturn(PETSC_SUCCESS);
}


/* - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 

     Private functions for debugging

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - */

/*
    Debug: save vector to text file
*/
PetscErrorCode petsc_debug_save_vec(const char *filename, Vec vec)
{   
    PetscFunctionBeginUser;

    PetscViewer viewer;
    PetscViewerASCIIOpen(MPI_COMM_WORLD, filename, &viewer);
    PetscViewerPushFormat(viewer, PETSC_VIEWER_ASCII_COMMON);
    VecView(vec, viewer);
    PetscViewerPopFormat(viewer);
    PetscViewerDestroy(&viewer);

    PetscFunctionReturn(PETSC_SUCCESS);
}

/*
    Debug: save matrix to text file
*/
PetscErrorCode petsc_debug_save_mat(const char *filename, Mat mat)
{   
    PetscFunctionBeginUser;

    PetscViewer viewer;
    PetscViewerASCIIOpen(MPI_COMM_WORLD, filename, &viewer);
    PetscViewerPushFormat(viewer, PETSC_VIEWER_ASCII_DENSE);
    MatView(mat, viewer);
    PetscViewerPopFormat(viewer);
    PetscViewerDestroy(&viewer);

    PetscFunctionReturn(PETSC_SUCCESS);
}

/////////////////////////////////////////////////////////////////
//                   P e t s c I m p l                         //
/////////////////////////////////////////////////////////////////

//-----------
// PetscImpl 
//-----------
// The PetscImpl private class hides PETSc data structures
// and functions.
//
class PetscLinearAlgebra::PetscImpl {
  public:
    // True if PESc has been initialized.
    static bool pesc_initialized;

    PetscImpl();
    void initialize(ComMod& com_mod, eqType& lEq);
    void set_preconditioner(consts::PreconditionerType prec_type);
    void solve(ComMod& com_mod, eqType& lEq, const Vector<int>& incL, const Vector<double>& res);
    void init_dir_and_coupneu_bc(ComMod& com_mod, const Vector<int>& incL, const Vector<double>& res);

    consts::PreconditionerType preconditioner_;

    // Local to global mapping
    Vector<int> ltg_;

    // Factor for Dirichlet BCs
    Array<double> W_;

    // Residue
    Array<double> R_;

    // Factor for Lumped Parameter BCs
    Array<double> V_;

};

bool PetscLinearAlgebra::PetscImpl::pesc_initialized = false;

PetscLinearAlgebra::PetscImpl::PetscImpl()
{
}

//-------------------------
// init_dir_and_coupneu_bc
//-------------------------
//
void PetscLinearAlgebra::PetscImpl::init_dir_and_coupneu_bc(ComMod& com_mod, 
    const Vector<int>& incL, const Vector<double>& res)
{
  using namespace consts;
  using namespace fsi_linear_solver;

  int dof = com_mod.dof;
  auto& lhs = com_mod.lhs;

  if(lhs.nFaces != 0) {
    for (auto& face : lhs.face) {
      face.incFlag = true;
    }

    for (int faIn = 0; faIn < lhs.nFaces; faIn++) {
      if (incL(faIn) == 0)  {
        lhs.face[faIn].incFlag = false;
      }
    }

    for (int faIn = 0; faIn < lhs.nFaces; faIn++) {
      auto& face = lhs.face[faIn];
      face.coupledFlag = false;
      if (!face.incFlag) {
        continue;
      }

      bool flag = (face.bGrp == BcType::BC_TYPE_Neu);

      if (flag && res(faIn) != 0.0) {
        face.res = res(faIn);
        face.coupledFlag = true;
      }
    }
  }

  W_ = 1.0;

  for (int faIn = 0; faIn < lhs.nFaces; faIn++) {
    auto& face = lhs.face[faIn];
    if (!face.incFlag) {
      continue;
    }

    int faDof = std::min(face.dof,dof);

    if (face.bGrp == BcType::BC_TYPE_Dir) {
      for (int a = 0; a < face.nNo; a++) {
        int Ac = face.glob(a);
        for (int i = 0; i < faDof; i++) {
          W_(i,Ac) = W_(i,Ac) * face.val(i,a);
        }
      }
    }
  }

  V_ = 0.0;
  bool isCoupledBC = false;

  for (int faIn = 0; faIn < lhs.nFaces; faIn++) {
    auto& face = lhs.face[faIn];
    if (face.coupledFlag) {
      isCoupledBC = true;
      int faDof = std::min(face.dof,dof);

      for (int a = 0; a < face.nNo; a++) {
        int Ac = face.glob(a);
        for (int i = 0; i < faDof; i++) {
          V_(i,Ac) = V_(i,Ac) + sqrt(fabs(res(faIn))) * face.val(i,a);
        }
      }
    }
  }
}

//------------
// initialize
//------------
// Initialize PETSc and create a linear solver.
//
void PetscLinearAlgebra::PetscImpl::initialize(ComMod& com_mod, eqType& equation)
{
  if (!pesc_initialized) {
    petsc_destroy_all(com_mod.nEq);

    petsc_initialize(com_mod.lhs.nNo, com_mod.lhs.mynNo, com_mod.lhs.nnz, com_mod.nEq, 
        com_mod.ltg.data(), com_mod.lhs.map.data(), com_mod.lhs.rowPtr.data(), 
        com_mod.lhs.colPtr.data(), com_mod.eq[0].ls.config.data());

    pesc_initialized = true;
  }

  auto prec_type = preconditioner_;
  auto ls_type = equation.ls.LS_type;
  auto phys = equation.phys;
  auto& ls = equation.ls;

  // Find the equation number of `equation` in com_mod.eq[].
  //
  int eq_num = -1;

  for (int a = 0; a < com_mod.nEq; a++) {
    if (std::addressof(com_mod.eq[a]) == std::addressof(equation)) {
      eq_num = a;
    }
  }

  petsc_create_linearsolver(ls_type, prec_type, ls.sD, ls.mItr, ls.relTol, ls.absTol, 
      phys, equation.dof, eq_num, com_mod.nEq);
}

void PetscLinearAlgebra::PetscImpl::set_preconditioner(consts::PreconditionerType prec_type)
{
  preconditioner_ = prec_type;
}

//-------
// solve
//-------
//
void PetscLinearAlgebra::PetscImpl::solve(ComMod& com_mod, eqType& lEq, const Vector<int>& incL, const Vector<double>& res)
{
  W_.resize(com_mod.dof, com_mod.tnNo);
  V_.resize(com_mod.dof, com_mod.tnNo);

  init_dir_and_coupneu_bc(com_mod, incL, res);

  // only excute once for each equation
  //
  petsc_create_linearsystem(com_mod.dof, com_mod.cEq, com_mod.nEq, W_.data(), V_.data());

  petsc_set_values(com_mod.dof, com_mod.cEq, com_mod.R.data(), com_mod.Val.data(), W_.data(), V_.data());

  petsc_solve(&lEq.FSILS.RI.fNorm, &lEq.FSILS.RI.iNorm, &lEq.FSILS.RI.dB, &lEq.FSILS.RI.callD, 
      &lEq.FSILS.RI.suc, &lEq.FSILS.RI.itr, com_mod.R.data(), lEq.FSILS.RI.mItr, com_mod.dof, com_mod.cEq);

}

//-----------------------------------------------
// Block Nested Preconditioner
//-----------------------------------------------
//

BlockNestedPreconditioner::BlockNestedPreconditioner( 
    const double &rtol0, const double &atol0, 
    const double &dtol0, const int &maxit0,
    const double &rtol1, const double &atol1, 
    const double &dtol1, const int &maxit1)
{
    PetscInt  i, j, ii, jj;
    PetscInt *uindx, *pindx;

    // Create objects for internal linear solvers
    solver_0 = new BlockNestedPC_InternalLinearSolver( rtol0, atol0, dtol0, maxit0, 
        "ls0_", "pc0_" );

    solver_1 = new BlockNestedPC_InternalLinearSolver( rtol1, atol1, dtol1, maxit1,
        "ls1_", "pc1_" );

    // Create index set for velocity and pressure block 
    PetscCall(PetscMalloc2((dof-1)*plhs.mynNo, &uindx, plhs.mynNo, &pindx));
    for (i = 0; i < plhs.mynNo; i++){
        ii = (dof-1)*i;
        jj = dof*plhs.ltg[i];
        for (j = 0; j < dof-1; j++){
            uindx[ii+j] = jj + j;
        }
        pindx[i] = jj + dof - 1;
    }
    PetscCall(ISCreateGeneral(MPI_COMM_WORLD, (dof-1)*plhs.mynNo, uindx, PETSC_COPY_VALUES, &velocity_is));
    PetscCall(ISSetBlockSize(velocity_is, dof-1));
    PetscCall(ISSort(velocity_is));
    PetscCall(ISCreateGeneral(MPI_COMM_WORLD, plhs.mynNo, pindx, PETSC_COPY_VALUES, &pressure_is));
    PetscCall(ISSort(pressure_is));

    // Initialize block matrices to NULL
    A_00 = NULL;
    A_01 = NULL;
    A_10 = NULL;
    A_11 = NULL;

    // Initialize r_0, r_1 and z_0, z_1 based on velocity index set
    PetscInt local_size_v, local_size_p;
    PetscCall(ISGetLocalSize(velocity_is, &local_size_v));
    PetscCall(VecCreateMPI(MPI_COMM_WORLD, local_size_v, PETSC_DECIDE, &r_0));
    PetscCall(VecDuplicate(r_0, &z_0)); 

    PetscCall(ISGetLocalSize(pressure_is, &local_size_p));
    PetscCall(VecCreateMPI(MPI_COMM_WORLD, local_size_p, PETSC_DECIDE, &r_1));
    PetscCall(VecDuplicate(r_1, &z_1)); 

    PetscCall(PetscFree2(uindx, pindx));
}

BlockNestedPreconditioner::~BlockNestedPreconditioner()
{
  // Clean up internal linear solvers
  delete solver_0; delete solver_1;
  // Clean up index sets
  ISDestroy(&velocity_is); ISDestroy(&pressure_is);
  // Clean up block matrices
  MatDestroy(&A_00); MatDestroy(&A_01); MatDestroy(&A_10); MatDestroy(&A_11);
  // Clean up sub-vectors
  VecDestroy(&r_0); VecDestroy(&r_1);
  VecDestroy(&z_0); VecDestroy(&z_1);
}

PetscErrorCode BlockNestedPreconditioner::BlockNestedPC_Apply(PC pc, Vec r, Vec z) 
{
    // Split the residual vector r into velocity (r_0) and pressure (r_1) components
    PetscCall(VecGetSubVector(r, velocity_is, &r_0)); 
    PetscCall(VecGetSubVector(r, pressure_is, &r_1));

    // Intermediate solver
    //-------------------------------------------------
    // z_0 = A_00^{-1} * r_0 -> solver_0
    //-------------------------------------------------
    PC pc_0;
    PetscCall(solver_0->GetPC(&pc_0));
    PetscCall(PCSetType(pc_0, PCHYPRE));
    PCHYPRESetType(pc, "boomeramg");
    PetsCall(solver_0->SetOperators(A_00));
    PetscCall(solver_0->Solve(r_0, z_0, PETSC_FALSE));

    //-------------------------------------------------
    // r_1 - A_10 * z_0
    //-------------------------------------------------
    PetscCall(MatMult(A_10, z_0, z_0));
    z_1 = r_1 - z_0;

    //-------------------------------------------------
    // S * z_1 = r_1 - A_10 * z_0 -> inner solver
    // S = A_11 - A_10 * A_00^{-1} * A_01
    // matrix-free alghorithm for S involves application
    // of S to a vector x = r_1 - A_10 * z_0 and using 
    // GMRES solver to solve for z_1
    //-------------------------------------------------
    PC pc_1;
    PetscCall(solver_1->GetPC(&pc_1));
    PetscCall(PCSetType(pc_1, PCHYPRE));
    PCHYPRESetType(pc, "boomeramg");
    solver_1->SchurComplement = new MatrixFreeSchurComplement(A_00, A_01, A_10, A_11, solver_0);
    PetscCall(solver_1->SchurComplement->ApplySchurComplement());
    PetscCall(solver_1->Setoperators(solver_1->SchurComplement->S));
    PetscCall(sovler_1->Solve(z_1, z_1, PETSC_FALSE));

    // Update the momentum residual: r_0 = r_0 - A_01 * z_1
    PetscCall(MatMul(A_01, z_1, z_0));
    PetscCall(VecWAXPY(z_0, -1.0, z_0, r_0));

    // Solve A_00 * z_0 = r_0
    PetscCall(solver_0->solve(z_0, z_0, PETSC_FALSE));

     // Combine results back into the full vector z = PC^{-1} * r
    PetscCall(VecSetValues(z, velocity_is, z_0, INSERT_VALUES)); // Velocity result
    PetscCall(VecSetValues(z, pressure_is, z_1, INSERT_VALUES)); // Pressure result
    PetscCall(VecAssemblyBegin(z));
    PetscCall(VecAssemblyEnd(z));

    // Cleanup sub-vectors
    PetscCall(VecRestoreSubVector(r, velocity_is, &r_0));
    PetscCall(VecRestoreSubVector(r, pressure_is, &r_1));

    PetscFunctionReturn(0);
}

PetscErrorCode BlockNestedPreconditioner::BlockNestedPC_SetMatrix(Mat A) {
    
    tangent_matrix = A;

    if (A_00 == NULL) {
        // Extract submatrices for the first time
        MatGetSubMatrix(A, velocity_is, velocity_is, MAT_INITIAL_MATRIX, &A_00);
        MatGetSubMatrix(A, velocity_is, pressure_is, MAT_INITIAL_MATRIX, &A_01);
        MatGetSubMatrix(A, pressure_is, velocity_is, MAT_INITIAL_MATRIX, &A_10);
        MatGetSubMatrix(A, pressure_is, pressure_is, MAT_INITIAL_MATRIX, &A_11);
    } else {
        // Reuse existing submatrices by updating their values
        MatGetSubMatrix(A, velocity_is, velocity_is, MAT_REUSE_MATRIX, &A_00);
        MatGetSubMatrix(A, velocity_is, pressure_is, MAT_REUSE_MATRIX, &A_01);
        MatGetSubMatrix(A, pressure_is, velocity_is, MAT_REUSE_MATRIX, &A_10);
        MatGetSubMatrix(A, pressure_is, pressure_is, MAT_REUSE_MATRIX, &A_11);
    }

    return 0;
}
//-------------------------------------------------------------------------
// Internal Linear solver for the Block Nested Preconditioner: 
// solver_0, solver_1
//-------------------------------------------------------------------------

BlockNestedPC_InternalLinearSolver::BlockNestedPC_InternalLinearSolver()
: rtol( 1.0e-5 ), atol( 1.0e-50 ), dtol( 1.0e50 ), maxits(10000)
{
    KSPCreate(MPI_COMM_WORLD, &ksp);
    KSPSetTolerances(ksp, rtol, atol, dtol, maxits);
    KSPSetType(ksp, KSPGMRES);
    KSPSetFromOptions(ksp);
} 

BlockNestedPC_InternalLinearSolver::BlockNestedPC_InternalLinearSolver(
    const double &input_rtol, const double &input_atol,
    const double &input_dtol, const int &input_maxits)
: rtol( input_rtol ), atol( input_atol ),
  dtol( input_dtol ), maxits( input_maxits )
{
    KSPCreate(MPI_COMM_WORLD, &ksp);
    KSPSetTolerances(ksp, rtol, atol, dtol, maxits);
    KSPSetType(ksp, KSPGMRES);
    KSPSetFromOptions(ksp);
}

BlockNestedPC_InternalLinearSolver::BlockNestedPC_InternalLinearSolver(
    const double &input_rtol, const double &input_atol,
    const double &input_dtol, const int &input_maxits, 
    const char * const &ksp_prefix, const char * const &pc_prefix )
: rtol( input_rtol ), atol( input_atol ),
  dtol( input_dtol ), maxits( input_maxits )
{
    KSPCreate(MPI_COMM_WORLD, &ksp);
    KSPSetTolerances(ksp, rtol, atol, dtol, maxits);
    KSPSetType(ksp, KSPGMRES);
    KSPSetOptionsPrefix( ksp, ksp_prefix );

    PC ksp_pc;
    KSPGetPC( ksp, &ksp_pc );
    PCSetOptionsPrefix( ksp_pc, pc_prefix );
  
    KSPSetFromOptions(ksp);
    PCSetFromOptions(ksp_pc);
}

BlockNestedPC_InternalLinearSolver::~BlockNestedPC_InternalLinearSolver()
{
  KSPDestroy(&ksp);
}

void BlockNestedPC_InternalLinearSolver::Solve( const Vec &G, Vec &out_sol, const bool &isPrint )
{
  KSPSolve(ksp, G, out_sol);

  if( isPrint )
  {
    PetscInt its;
    KSPGetIterationNumber(ksp, &its);
    PetscReal resnorm;
    KSPGetResidualNorm(ksp, &resnorm);
    PetscPrintf(MPI_COMM_WORLD, "  --- KSP: %d, %e", its, resnorm);
  }
}

void BlockNestedPC_InternalLinearSolver::Solve( const Mat &K, const Vec &G, Vec &out_sol,
   const bool &isPrint )
{
  KSPSetOperators(ksp, K, K);
  KSPSolve(ksp, G, out_sol);

  if( isPrint )
  {
    PetscInt its;
    KSPGetIterationNumber(ksp, &its);
    PetscReal resnorm;
    KSPGetResidualNorm(ksp, &resnorm);
    PetscPrintf(MPI_COMM_WORLD, "  --- KSP: %d, %e", its, resnorm);
  }
}

void BlockNestedPC_InternalLinearSolver::Monitor() const
{
  PetscViewerAndFormat *vf;
  PetscViewerAndFormatCreate(PETSC_VIEWER_STDOUT_WORLD,PETSC_VIEWER_DEFAULT,&vf);
  KSPMonitorSet(ksp,(PetscErrorCode (*)(KSP,PetscInt,PetscReal,void*))KSPMonitorDefault,vf,(PetscErrorCode (*)(void**))PetscViewerAndFormatDestroy);
}

MatrixFreeSchurComplement::MatrixFreeSchurComplement( Mat A_00, Mat A_01, Mat A_10, Mat A_11, 
    BlockNestedPC_InternalLinearSolver* solver_0 )
{
    PetscInt m, n;

    // Initialize the matrix-free context
    PetscCall(PetscMalloc1(1, &MatShellCtx));

    MatShellCtx->A00 = A_00;
    MatShellCtx->A01 = A_01;
    MatShellCtx->A10 = A_10;
    MatShellCtx->A11 = A_11;

    MatShellCtx->ASolver = solver_0;

    // Define the size of the Schur complement
    PetscCall(MatGetSize(MatShellCtx->A11, &m, &n));

    // Create the shell matrix for the Schur complement
    PetscCall(MatCreateShell(MPI_COMM_WORLD, m, n, PETSC_DECIDE, PETSC_DECIDE, MatShellCtx, &S));
}

MatrixFreeSchurComplement::~MatrixFreeSchurComplement()
{
    PetscCall(MatDestroy(&S));
    PetscCall(PetscFree(MatShellCtx));
}

// Method to set the Schur complement operation
PetscErrorCode MatrixFreeSchurComplement::ApplySchurComplement() {
    PetscFunctionBeginUser;
    PetscCall(MatShellSetOperation(S, MATOP_MULT, (void (*)(void))SchurComplMult));
    PetscFunctionReturn(PETSC_SUCCESS);
}

// Custom matrix-vector multiplication function for Schur complement
// y = mat * x
// mat -> Schur complement matrix (not explicitly defined)
// x -> input vector ( it is the residual from the GMRES )
// y -> output vector ( it will form the krylov subspace in the GMRES )
PetscErrorCode SchurComplMult(Mat mat, Vec x, Vec y) {

    Vec a11xp, a01xp, a00inv_a01xp;
    
    PetscFunctionBeginUser;

    ShellMatrixContext *ctx;
    PetscCall(MatShellGetContext(mat, &ctx));

    // Krylove subspace: S * x
    // A11 * x
    PetscCall(MatMult(ctx->A11, x, a11xp));

    // A01 * x
    PetscCall(MatMult(ctx->A01, x, a01xp));

    // A00^{-1} * (A01 * x)
    ctx->ASolver->Solve(a01xp, a00inv_a01xp, PETSC_FALSE);

    // (A10 * A00^{-1} * A01) * x
    PetscCall(MatMult(ctx->A10, a00inv_a01xp, a01xp));

    // y = ( A11 - A10 * A00^{-1} * A01 ) * x
    PetscCall(VecWAXPY(y, -1.0, a01xp, a11xp)); 

    PetscCall(VecDestroy(&a11xp, &a01xp, &a00inv_a01xp));   

    PetscFunctionReturn(PETSC_SUCCESS);
}